<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
  hyphens: none;
}

.reveal h3 {
  font-size: 1.4em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.smallcode pre code {
  font-size: 0.9em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

.reveal .mediumtext {
  font-size: 0.85em;
}

</style>


Dynamic forecasts - with Bayesian linear models and neural networks
========================================================
author: Sigrid Keydana, Trivadis
date: 2017/14/11
autosize: true
incremental:false
width: 1600
height: 900

About me & my employer
========================================================
class:mediumtext


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Trivadis
- DACH-based IT consulting and service company, from traditional technologies to big data/machine learning/data science

My background
- from psychology/statistics via software development and database engineering to data science and ML/DL

My passion
- machine learning and deep learning
- data science and (Bayesian) statistics
- explanation/understanding over prediction accuracy

Where to find me
- blog: http://recurrentnull.wordpress.com
- twitter: @zkajdan


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Dynamic models for timeseries prediction, - why?
</h1>


Our task today: forecast men's 400m Olympic winning times
========================================================

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=16, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE,
                      cache = TRUE)

options(digits = 4, scipen = 4)
```

```{r}
library(ggplot2)
library(dplyr)
library(forecast)
library(tidyr)
library(keras)
library(R.matlab)
library(dlm)
library(quantmod)
library(gridExtra)
library(readr)
source("functions.R")

bold_text_20 <- element_text(face = "bold", size = 20)
bold_text_16 <- element_text(face = "bold", size = 16)
```


It's 2000, just before the Olympics. This is the data we have:

```{r}
data <- readMat("olympics.mat")
male400 <- data$male400[ ,1:2]
male400 <- as.data.frame(male400) %>% rename(year = V1, seconds = V2)
male400 <- male400 %>% bind_rows(
  c(year = 2012, seconds = 43.94), c(year = 2016, seconds = 43.03))

# data up till 1996
male400_1996 <- male400 %>% filter(year < 1997)

# data from 2000
male400_2000 <- male400 %>% filter(year > 1997)

ggplot(male400_1996, aes(x = year, y = seconds)) + geom_line() + ggtitle("Men's 400m Olympic winning times 1986-1996") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)


```




This looks like we might even try linear regression...
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

```{r, echo=TRUE}
lm(seconds ~ year, male400_1996) %>% coefficients()
```

```{r}
fit <- lm(seconds ~ year, male400_1996)
df <- male400_1996 %>% bind_rows(data.frame(year = seq(2000, 2016, by=4)))
preds <- fit %>% predict(newdata = df[25:29, ], interval = "prediction")
df <- df %>% bind_cols(data.frame(pred = c(rep(NA, 24), preds[ , 1]),
                                  lwr = c(rep(NA, 24), preds[ , 2]),
                                  upr = c(rep(NA, 24), preds[ , 3])))

ggplot(df, aes(x = year, y=seconds)) + geom_line() + 
  geom_point(aes(y = pred), color = "green", size = 3) +
  geom_ribbon(aes(x = year, ymin = lwr, ymax = upr), alpha = 0.1) +
  ggtitle("Men's 400m Olympic winning times 1986-1996 and predictions for 2000-2016") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")
```


Unfortunately...
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

```{r}
fit <- lm(seconds ~ year, male400_1996)
df <- male400 
preds <- fit %>% predict(newdata = df[25:29, ], interval = "prediction")
df <- df %>% bind_cols(data.frame(pred = c(rep(NA, 24), preds[ , 1]),
                                  lwr = c(rep(NA, 24), preds[ , 2]),
                                  upr = c(rep(NA, 24), preds[ , 3])))

ggplot(df, aes(x = year, y=seconds)) + geom_line() + 
  geom_point(aes(y = pred), color = "red", size = 3) +
  geom_ribbon(aes(x = year, ymin = lwr, ymax = upr, alpha = 0.1)) +
  ggtitle("Men's 400m Olympic winning times 1986-2016 and predictions for 2000-2016") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")
```

OK then... let's just use ARIMA
========================================================
class:smallcode

&nbsp;

(yes, the series is not totally regular, but let me make the point ;-))

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

```{r, echo=TRUE}
arima_fit <- auto.arima(male400_1996$seconds)
arima_fit$coef
```


```{r}
autoplot(forecast(arima_fit, h=5)) + 
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16, legend.position = "none")
```



What if... 
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

What if we didn't have to:

- use static coefficients?

or even:
- model linear relationships?


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



No static coefficients needed: Dynamic Linear Models (DLMs)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src='dlm_diag.png' width = 50%/>
</figure>


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Deep and non-linear: LSTM networks
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src='lstm_olah2.png' width='80%'/>
    <figcaption>Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</figcaption>
</figure>



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Dynamic linear models
</h1>


State space models
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src="hmm.png" width='40%'/>
    <figcaption>Source: Wikipedia</figcaption>
</figure>

&nbsp;

&nbsp;

States form a Markov chain.

Observations are independent conditional on states.

Hidden states, observations, and time
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- Hidden states evolve in time (state equation):

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\theta_t = G_t\theta_{t-1} + w_t$

- Hidden states generate observables (observation equation):

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Y_t = F_t\theta_t + v_t$

- System noise is normally distributed with mean $0$ and variance $W_t$: 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_t \sim \mathcal{N}(0, W_t)$

- Observation noise is normally distributed with mean $0$ and variance $V_t$: 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $v_t \sim \mathcal{N}(0, V_t)$



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


How is this dynamic?
========================================================
class:smallcode

&nbsp;

In theory, any and all of 

- system noise
- observation noise
- state equation
- observation equation 

could change over time.


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;


Example: Where's the unicorn?
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- All we have is a sequence of (noisy) observations
- And a sequence of hypothesized true positions (states)

How can we optimise our assumptions about the true position?

<figure>
    <img src='unicorn.png' style="opacity: 0.1;" width="80%"/>
</figure>

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Bayesian updating
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- At any time, we have a current best estimate on where the unicorn is (given past observations)
- We compute the next true position (state) based on the _state equation_ (this is the _prior_)
- We predict the corresponding observation based on the _observation equation_ (this is the _likelihood_)
- As soon as the new observation arrives, we can correct our estimate and compute the _posterior_ on the state

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Show me the math: Kalman filter
========================================================
class:smalltext

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />

&nbsp;

Here's the model again

- state equation $\theta_t = G_t\theta_{t-1} + w_t$, where $w_t \sim \mathcal{N}(0, W_t)$
- observation equation $Y_t = F_t\theta_t + v_t$, where $v_t \sim \mathcal{N}(0, V_t)$
- prior $\theta_0 \sim \mathcal{N}_p(m_0, C_0)$

&nbsp;

Kalman steps (given: $\theta_{t-1}|y_{1:t-1} \sim \mathcal{N}(m_{t-1}, C_{t-1})$)

  1. One-step-ahead predictive distribution of $\theta_t$ given $y_{1:t-1}$:
  
  $E(\theta_t|y_{1:t-1}) = a_t = G_tm_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Var(\theta_t|y_{1:t-1}) = R_t = G_tC_{t-1}G'_t + W_t$

  2.  One-step-ahead predictive distribution of $Y_t$ given $y_{1:t-1}$:
  
  $E(Y_t|y_{1:t-1}) = f_t = F_t a_t$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Var(Y_t|y_{1:t-1}) = Q_t = F_tR_tF'_t + V_t$

  3. Filtering distribution of $\theta_t$ given $y_{1:t}$:
  
  $E(\theta_t|y_{1:t}) = m_t = a_t + R_tF'_tQ_t^{-1}e_t$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Var(\theta_t|y_{1:t}) = C_t = R_t - R_tF'_tQ_t^{-1}F_tR_t$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; where $e_t =  Y_t - f_t$ is the forecast error.



Kalman filter in a nutshell
========================================================
class:smallcode


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

> We correct our prediction by how much it differs from the incoming observation.

&nbsp;
&nbsp;

Posterior estimate &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $E(\theta_t|y_{1:t}) = m_t = a_t + R_tF'_tQ_t^{-1}e_t$


Kalman gain &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $R_tF'_tQ_t^{-1}$


Aside: Filtering vs. smoothing
========================================================
class:smallcode

&nbsp;

- In _filtering_, we base our estimates of the true states on all observations made _so far_.
- In _smoothing_, we form our opinion about the states "in hindsight", based on all past and (available) future states.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;



Local level model ("random walk plus noise")
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

State equation: $\mu_t = \mu_{t-1} + w_t, w_t \sim \mathcal{N}(0, W)$

Observation equation: $Y_t = \mu_t + v_t, v_t \sim \mathcal{N}(0, V)$

$G = F = 1$

&nbsp;

&nbsp;

W and V could be time-invariant or time-dependent.


Local level example
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;



```{r}
lakeSup <- ts(read.table("Datasets/lakeSuperior.dat", skip = 3,
                         colClasses = "numeric")[, 2], start = 1900)
```

```{r, echo=TRUE}
library(dlm)

build <- function(params) {
  dlmModPoly(order = 1, dV = exp(params[1]), dW = exp(params[2]))
}
initial_params <- rep(log(var(diff(lakeSup))), 2)
mle <- dlmMLE(lakeSup, initial_params, build)

model <- dlmModPoly(order = 1, dV = exp(mle$par[1]), dW = exp(mle$par[2]))
Map(function(x) round(x,2), unlist(model)) %>% unlist()
```


```{r, fig.width=10, fig.height=5}
autoplot(lakeSup) + ggtitle("Annual precipitation at Lake Superior, 1990-1986") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)

```


&nbsp;


Local level example: Filtering and smoothing
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;


```{r}
filt <- dlmFilter(lakeSup, model)
smooth <- dlmSmooth(lakeSup, model)
df <- data.frame(year = time(lakeSup),
                 actual = unclass(lakeSup),
                 filtered = dropFirst(filt$m),
                 smoothed = dropFirst(smooth$s))
df <- gather(df, key = type, value = value, -year)
ggplot(df, aes(x = year, y = value, color = type)) + geom_line() + 
  ggtitle("Annual precipitation at Lake Superior: filtering and smoothing estimates") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```


Two types of uncertainty
========================================================
class:smallcode

&nbsp;

How would the filtering and smoothing curves have looked had we specified equal degrees of system and observation noise?

&nbsp;

```{r, echo = TRUE}
model2 <- dlmModPoly(order = 1, dV = 10, dW = 10)
filt2 <- dlmFilter(lakeSup, model2)
smooth2 <- dlmSmooth(lakeSup, model2)
```

```{r}
df <- data.frame(year = time(lakeSup),
                 actual = unclass(lakeSup),
                 filtered = dropFirst(filt2$m),
                 smoothed = dropFirst(smooth2$s))
df <- gather(df, key = type, value = value, -year)
ggplot(df, aes(x = year, y = value, color = type)) + geom_line() + 
  ggtitle("Annual precipitation at Lake Superior: filtering and smoothing estimates") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;




Local level example: Forecasting 
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

```{r}
build <- function(params) {
  dlmModPoly(order = 1, dV = exp(params[1]), dW = exp(params[2]))
}
initial_params <- rep(log(var(diff(lakeSup))), 2)
mle <- dlmMLE(lakeSup, initial_params, build)

model <- dlmModPoly(order = 1, dV = exp(mle$par[1]), dW = exp(mle$par[2]))
filt <- dlmFilter(lakeSup, model)
fc <- dlmForecast(filt, nAhead = 10)
fc_sd <- sqrt(unlist(fc$Q))
lower <- fc$f - qnorm(0.025, lower = FALSE) * fc_sd
upper <- fc$f + qnorm(0.025, lower = FALSE) * fc_sd
df <- data.frame(year = c(time(lakeSup), 1987:1996),
                 value = c(unclass(lakeSup), rep(NA,10)),
                 forecast = c(rep(NA,87), fc$f),
                 lower = c(rep(NA,87), lower),
                 upper = c(rep(NA,87), upper))
ggplot(df, aes(x = year, y = value)) + geom_line() + 
  geom_line(aes(y = forecast), color = "red") + 
  geom_ribbon(aes(ymin = df$lower, ymax = df$upper), alpha = .3) +
  ggtitle("Annual precipitation at Lake Superior: forecasts with 95% prediction intervals") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```




Another local level example: Nvidia stock prices
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



```{r}
nvda <- getSymbols("NVDA", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$NVDA.Adjusted
index <- index(nvda)
nvda <- as.ts(nvda)
log_var_diff <- log(var(diff(nvda)))
```

```{r, echo=TRUE}
build <- function(params) {
  dlmModPoly(order = 1, dV = exp(params[1]), dW = exp(params[2]))
}
initial_params <- rep(log_var_diff, 2)
mle <- dlmMLE(nvda, initial_params, build)

model <- dlmModPoly(order = 1, dV = exp(mle$par[1]), dW = exp(mle$par[2]))
Map(function(x) round(x,2), unlist(model)) %>% unlist()
```


```{r, fig.width=10, fig.height=5}
autoplot(nvda) + ggtitle("Nvidia stock prices, Jan.-Sept. 2017") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
```


Nvidia: filtering and smoothing
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


```{r}
filt <- dlmFilter(nvda, model)
smooth <- dlmSmooth(nvda, model)
df <- data.frame(day = time(nvda),
                 actual = unclass(nvda),
                 filtered = dropFirst(filt$m),
                 smoothed = dropFirst(smooth$s))
df <- gather(df, key = type, value = value, -day)
ggplot(df, aes(x = day, y = value, color = type)) + geom_line() + 
  ggtitle("Nvidia stock prices: filtering and smoothing estimates") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```


(No, the lines did not turn invisible...)


Nvidia: Forecasting (or, let's beat the market)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

```{r}
build <- function(params) {
  dlmModPoly(order = 1, dV = exp(params[1]), dW = exp(params[2]))
}
initial_params <- rep(log_var_diff, 2)
mle <- dlmMLE(nvda, initial_params, build)

model <- dlmModPoly(order = 1, dV = exp(mle$par[1]), dW = exp(mle$par[2]))
filt <- dlmFilter(nvda, model)

fc <- dlmForecast(filt, nAhead = 5)
fc_sd <- sqrt(unlist(fc$Q))
lower <- fc$f - qnorm(0.025, lower = FALSE) * fc_sd
upper <- fc$f + qnorm(0.025, lower = FALSE) * fc_sd
df <- data.frame(day = c(time(nvda), 189:193),
                 actual = c(unclass(nvda), rep(NA,5)),
                 forecast = c(rep(NA,188), fc$f),
                 lower = c(rep(NA,188), lower),
                 upper = c(rep(NA,188), upper))
ggplot(df, aes(x = day, y = actual)) + geom_line() + 
  geom_line(aes(y = forecast), color = "red") + 
  geom_ribbon(aes(ymin = df$lower, ymax = df$upper), alpha = .3) +
  ggtitle("Nvidia: forecasts with 95% prediction intervals") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```


Not with this we won't...


DLMs - besides the local level model
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- linear trend model 
- seasonal models 
- regression models (with X as observation-generating matrix F and the coefficients as hidden state)
- multivariate models, exploiting underlying commonalities ("seemingly unrelated time series equations")
- multivariate regression models ("seemingly unrelated regression models")

Again, system and observation noise (as well as the transition equations) could be time-invariant or changing over time.


Nvidia revisited: Exploring relationships
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Comparing Nvidia to AMD, Mu, and IBM:

```{r}
amd <- getSymbols("AMD", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$AMD.Adjusted
nvda <- getSymbols("NVDA", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$NVDA.Adjusted
mu <- getSymbols("MU", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$MU.Adjusted
ibm <- getSymbols("IBM", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$IBM.Adjusted

df <- data.frame(index(amd), amd, mu, ibm, nvda)
tss <- read.zoo(df)
autoplot(tss) + facet_grid(Series ~ ., scales = "free_y") + ggtitle("")  +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  

```


Does it look like Nvidia and IBM have a somewhat "complementary" relationship?


Nvidia and IBM: Linear regression
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;


```{r, echo=TRUE}
lm(nvda ~ ibm) %>% summary()
```


So there's a strong relationship...
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

... but is it really _constant over time_?

```{r, echo=TRUE}
lm(nvda ~ ibm) %>% residuals() %>% qqnorm()
```

&nbsp;





DLM regression to the rescue!
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

```{r, echo=TRUE}
build <- function(u) {
  dlmModReg(ibm, dV = exp(u[1]), dW = exp(u[2 : 3]))
}
mle <- dlmMLE(nvda, parm = rep(0, 3), build)
model <- build(mle$par)

model$FF
model$GG
model$W
model$V
```



Dynamic regression coefficients
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Final coefficients:

```{r}
filt <-dlmFilter(nvda, model)
filt$m[1 + length(nvda), ] # latest coefficients
```

&nbsp;

Development of coefficients over time (except initial noise):

```{r}
intercept <- autoplot(as.ts(filt$m[ ,1])) + ggtitle("Intercept over time")  + ylab("intercept") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16) + coord_cartesian(ylim=c(90,200))
slope <-  autoplot(as.ts(filt$m[ ,2])) + ggtitle("Slope over time")  + ylab("slope") + coord_cartesian(ylim=c(-0.5, 0.5)) +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
grid.arrange(intercept, slope, ncol=2)
```


Back to the Olympics question...
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Let's model this as a dynamic regression problem!

```{r, echo=TRUE}
build <- function(u) {
  dlmModReg(male400_1996$year, dV = exp(u[1]), dW = exp(u[2 : 3]))
}
mle <- dlmMLE(male400_1996$seconds, parm = rep(0, 3), build)
model <- build(mle$par)
filt <-dlmFilter(male400_1996$seconds, model)
filt$m[1 + length(male400_1996$seconds), ] # most recent coefficients
```

```{r}
intercept <- autoplot(as.ts(filt$m[ ,1])) + ggtitle("Intercept over time")  + ylab("intercept") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16) + coord_cartesian(ylim=c(0,600))
slope <-  autoplot(as.ts(filt$m[ ,2])) + ggtitle("Slope over time")  + ylab("slope") + coord_cartesian(ylim=c(-0.35, 0.05)) + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
grid.arrange(intercept, slope, ncol=2)
```


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Deep Learning: Long Short Term Memory (LSTM)
</h1>


Learning temporal relationships: Recurrent Neural Networks
========================================================
class:mediumtext

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src='rnn1.png' width='80%'/>
    <figcaption>Source: Goodfellow et al. 2016, Deep Learning</figcaption>
</figure>


I need to forget: Long Short Term Memory
========================================================
class:mediumtext

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src='lstm.png' align='left' width='60%'/> <img src='lstm2.png' align='right' width='40%'/>
    <figcaption>Source: <a href='http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf'>Stanford CS 224D Deep Learning for NLP Lecture Notes</a></figcaption>
</figure>


Stock market again: Micron Technology
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


```{r}
mu <- getSymbols("MU", from="2015-01-01", to="2017-10-01", auto.assign = FALSE)$MU.Adjusted 
autoplot(mu) + ggtitle("Mu stock prices, Jan. 2015 -Sept. 2017") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
```


&nbsp;

Clearly remembering the downward walk won't help us to predict the near future...


One-step-ahead forecasts from LSTM
========================================================
class:smallcode

&nbsp;

```{r, results="hide"}
train <- window(mu, end = as.Date("2017-02-28")) %>% as.ts() %>% as.vector()
test <- window(mu, start = as.Date("2017-03-01")) %>% as.ts() %>% as.vector()

model_exists <- TRUE

lstm_num_timesteps <- 30
batch_size <- 1
epochs <- 50
lstm_units <- 32
model_type <- "model_lstm_simple"
lstm_type <- "stateless"
data_type <- "scaled"
test_type <- "MU"

model_name <- build_model_name(model_type, test_type, lstm_type, data_type, epochs)

minval <- min(train)
maxval <- max(train)

train <- normalize(train, minval, maxval)
test <- normalize(test, minval, maxval)

X_train <- build_X(train, lstm_num_timesteps) 
y_train <- build_y(train, lstm_num_timesteps) 

X_test <- build_X(test, lstm_num_timesteps) 
y_test <- build_y(test, lstm_num_timesteps) 

X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features)) %>% 
    layer_dense(units = 1) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  hist <- model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs,
    validation_data = list(X_test, y_test),
    callbacks = callback_early_stopping(patience=2)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

df <- data_frame(
  time_id = 1:692,
  train_ = c(denormalize(train, minval, maxval), rep(NA, length(test))),
  test_ = c(rep(NA, length(train)), denormalize(test, minval, maxval)),
  pred_train = c(rep(NA, lstm_num_timesteps), pred_train, rep(NA, length(test))),
  pred_test = c(rep(NA, length(train)), rep(NA, lstm_num_timesteps), pred_test)
)
df <- df %>% gather(key = 'type', value = 'value', train_:pred_test)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + ggtitle("Mu stock prices, Jan. 2015 -Sept. 2017, with one-step-ahead forecasts for training and test sets") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)

```

&nbsp;

Here, the sequence length (length of the hidden state) was chosen to be 30.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


How about multi-step forecasts?
========================================================
class:smallcode

&nbsp;

A way to get several predictions from an LSTM network in Keras: use TimeDistributed layer

&nbsp;

```{r, echo=TRUE, eval=FALSE}
model <- keras_model_sequential() 

model %>% 
  layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features), return_sequences = TRUE) %>% 
  time_distributed(layer_dense(units = 1)) %>% 
  compile(
    loss = 'mean_squared_error',
    optimizer = 'adam'
  )
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


What happens if we produce forecasts keeping the same state size?
========================================================
class:smallcode

&nbsp;

```{r, results="hide"}
train <- window(mu, end = as.Date("2017-02-28")) %>% as.ts() %>% as.vector()
test <- window(mu, start = as.Date("2017-03-01")) %>% as.ts() %>% as.vector()
all <- mu %>% as.ts() %>% as.vector()

num_train <- length(train)
num_test <- length(test)
num_all <- num_train + num_test

model_exists <- TRUE

lstm_num_predictions <- 30
lstm_num_timesteps <- 30
batch_size <- 1
epochs <- 500
lstm_units <- 32
lstm_type <- "stateless"
data_type <- "scaled"
test_type <- "MU"
model_type <- "model_lstm_time_distributed"

build_model_name = function(model_type, test_type, lstm_type, data_type, epochs) {
  paste(model_type, lstm_type, data_type, test_type, "timesteps", lstm_num_timesteps, sep="_")
}
model_name <- build_model_name(model_type, test_type, lstm_type, data_type, lstm_num_timesteps)

minval <- min(train)
maxval <- max(train)

train <- normalize(train, minval, maxval)
test <- normalize(test, minval, maxval)

matrix_train <- build_matrix(train, lstm_num_timesteps + lstm_num_predictions) 
matrix_test <- build_matrix(test, lstm_num_timesteps + lstm_num_predictions) 

X_train <- matrix_train[ ,1:lstm_num_timesteps]
y_train <- matrix_train[ ,(lstm_num_timesteps + 1):(lstm_num_timesteps + lstm_num_predictions)]

X_test <- matrix_test[ ,1:lstm_num_timesteps]
y_test <- matrix_test[ ,(lstm_num_timesteps + 1):(lstm_num_timesteps + lstm_num_predictions)]

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

y_train <- reshape_X_3d(y_train)
y_test <- reshape_X_3d(y_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features),
               return_sequences = TRUE) %>% 
    time_distributed(layer_dense(units = 1)) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs,
    validation_data = list(X_test, y_test), callbacks = callback_early_stopping(patience=2)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train <- pred_train[ , , 1]
pred_test <- pred_test[ , , 1]

df <- data_frame(time_id = 1:length(test),
                 test = denormalize(test, minval, maxval))
for(i in seq_len(nrow(pred_test))) {
  varname <- paste0("pred_test", i)
  df <- mutate(df, !!varname := c(rep(NA, lstm_num_timesteps),
                                  rep(NA, i-1),
                                  pred_test[i, ],
                                  rep(NA, num_test - lstm_num_predictions - lstm_num_timesteps -i +1)))
}

ggplot(df, aes(x = time_id, y =test)) + geom_line() +
  geom_line(aes(y = pred_test1), color = "cyan") + 
  geom_line(aes(y = pred_test30), color = "violet") + 
  geom_line(aes(y = pred_test60), color = "blue") + 
  geom_line(aes(y = pred_test90), color = "red") + 
  ggtitle("Mu stock prices test set, March 2017 - Sept. 2017, with selected 30-step-ahead forecasts") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)

```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Lowering state size to 12
========================================================
class:smallcode

&nbsp;

```{r, results="hide"}
train <- window(mu, end = as.Date("2017-02-28")) %>% as.ts() %>% as.vector()
test <- window(mu, start = as.Date("2017-03-01")) %>% as.ts() %>% as.vector()
all <- mu %>% as.ts() %>% as.vector()

num_train <- length(train)
num_test <- length(test)
num_all <- num_train + num_test

model_exists <- TRUE

lstm_num_predictions <- 12
lstm_num_timesteps <- 12
batch_size <- 1
epochs <- 500
lstm_units <- 32
lstm_type <- "stateless"
data_type <- "scaled"
test_type <- "MU"
model_type <- "model_lstm_time_distributed"

build_model_name = function(model_type, test_type, lstm_type, data_type, epochs) {
  paste(model_type, lstm_type, data_type, test_type, "timesteps", lstm_num_timesteps, sep="_")
}
model_name <- build_model_name(model_type, test_type, lstm_type, data_type, lstm_num_timesteps)

minval <- min(train)
maxval <- max(train)

train <- normalize(train, minval, maxval)
test <- normalize(test, minval, maxval)

matrix_train <- build_matrix(train, lstm_num_timesteps + lstm_num_predictions) 
matrix_test <- build_matrix(test, lstm_num_timesteps + lstm_num_predictions) 

X_train <- matrix_train[ ,1:lstm_num_timesteps]
y_train <- matrix_train[ ,(lstm_num_timesteps + 1):(lstm_num_timesteps + lstm_num_predictions)]

X_test <- matrix_test[ ,1:lstm_num_timesteps]
y_test <- matrix_test[ ,(lstm_num_timesteps + 1):(lstm_num_timesteps + lstm_num_predictions)]

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

y_train <- reshape_X_3d(y_train)
y_test <- reshape_X_3d(y_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features),
               return_sequences = TRUE) %>% 
    time_distributed(layer_dense(units = 1)) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs,
    validation_data = list(X_test, y_test), callbacks = callback_early_stopping(patience=2)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train <- pred_train[ , , 1]
pred_test <- pred_test[ , , 1]

df <- data_frame(time_id = 1:length(test),
                 test = denormalize(test, minval, maxval))
for(i in seq_len(nrow(pred_test))) {
  varname <- paste0("pred_test", i)
  df <- mutate(df, !!varname := c(rep(NA, lstm_num_timesteps),
                                  rep(NA, i-1),
                                  pred_test[i, ],
                                  rep(NA, num_test - lstm_num_predictions - lstm_num_timesteps -i +1)))
}

ggplot(df, aes(x = time_id, y =test)) + geom_line() +
  geom_line(aes(y = pred_test6), color = "cyan") + 
  geom_line(aes(y = pred_test26), color = "red") + 
  geom_line(aes(y = pred_test46), color = "violet") + 
  geom_line(aes(y = pred_test66), color = "blue") + 
  geom_line(aes(y = pred_test86), color = "green") +
  geom_line(aes(y = pred_test106), color = "pink") +
  geom_line(aes(y = pred_test126), color = "cyan")  + 
  ggtitle("Mu stock prices test set, March 2017 - Sept. 2017, with selected 12-step-ahead forecasts") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)

```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



... or even, to 6.
========================================================
class:smallcode

&nbsp;

```{r, results="hide"}
train <- window(mu, end = as.Date("2017-02-28")) %>% as.ts() %>% as.vector()
test <- window(mu, start = as.Date("2017-03-01")) %>% as.ts() %>% as.vector()
all <- mu %>% as.ts() %>% as.vector()

num_train <- length(train)
num_test <- length(test)
num_all <- num_train + num_test

model_exists <- TRUE

lstm_num_predictions <- 6
lstm_num_timesteps <- 6
batch_size <- 1
epochs <- 500
lstm_units <- 32
lstm_type <- "stateless"
data_type <- "scaled"
test_type <- "MU"
model_type <- "model_lstm_time_distributed"

build_model_name = function(model_type, test_type, lstm_type, data_type, epochs) {
  paste(model_type, lstm_type, data_type, test_type, "timesteps", lstm_num_timesteps, sep="_")
}
model_name <- build_model_name(model_type, test_type, lstm_type, data_type, lstm_num_timesteps)

minval <- min(train)
maxval <- max(train)

train <- normalize(train, minval, maxval)
test <- normalize(test, minval, maxval)

matrix_train <- build_matrix(train, lstm_num_timesteps + lstm_num_predictions) 
matrix_test <- build_matrix(test, lstm_num_timesteps + lstm_num_predictions) 

X_train <- matrix_train[ ,1:lstm_num_timesteps]
y_train <- matrix_train[ ,(lstm_num_timesteps + 1):(lstm_num_timesteps + lstm_num_predictions)]

X_test <- matrix_test[ ,1:lstm_num_timesteps]
y_test <- matrix_test[ ,(lstm_num_timesteps + 1):(lstm_num_timesteps + lstm_num_predictions)]

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

y_train <- reshape_X_3d(y_train)
y_test <- reshape_X_3d(y_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features),
               return_sequences = TRUE) %>% 
    time_distributed(layer_dense(units = 1)) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs,
    validation_data = list(X_test, y_test), callbacks = callback_early_stopping(patience=2)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train <- pred_train[ , , 1]
pred_test <- pred_test[ , , 1]

df <- data_frame(time_id = 1:length(test),
                 test = denormalize(test, minval, maxval))
for(i in seq_len(nrow(pred_test))) {
  varname <- paste0("pred_test", i)
  df <- mutate(df, !!varname := c(rep(NA, lstm_num_timesteps),
                                  rep(NA, i-1),
                                  pred_test[i, ],
                                  rep(NA, num_test - lstm_num_predictions - lstm_num_timesteps -i +1)))
}

ggplot(df, aes(x = time_id, y =test)) + geom_line() +
  geom_line(aes(y = pred_test8), color = "cyan") + 
  geom_line(aes(y = pred_test18), color = "red") + 
  geom_line(aes(y = pred_test28), color = "green") + 
  geom_line(aes(y = pred_test38), color = "violet") + 
  geom_line(aes(y = pred_test48), color = "blue") + 
  geom_line(aes(y = pred_test58), color = "brown") + 
  geom_line(aes(y = pred_test68), color = "orange")  +
  geom_line(aes(y = pred_test78), color = "cyan") + 
  geom_line(aes(y = pred_test88), color = "red") + 
  geom_line(aes(y = pred_test98), color = "violet") + 
  geom_line(aes(y = pred_test108), color = "blue") + 
  geom_line(aes(y = pred_test118), color = "green") + 
  geom_line(aes(y = pred_test128), color = "orange") + 
  geom_line(aes(y = pred_test138), color = "pink") + 
  ggtitle("Mu stock prices test set, March 2017 - Sept. 2017, with selected 6-step-ahead forecasts") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)

```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Clearly LSTM cannot show its true strength on a random walk...
========================================================
class:smallcode

&nbsp;

... let's give it some more adequate input!


```{r}
traffic_df <- read_csv("internet-traffic-data-in-bits-fr.csv", col_names = c("hour", "bits"), skip = 1)
ggplot(traffic_df, aes(x = hour, y = bits)) + geom_line() + ggtitle("Internet traffic  in bits, 7 June to 31 July 2005. Hourly data.") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)


```

This series has _multiple seasonality_.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Multiple seasonality
========================================================
class:smallcode

&nbsp;

- requires special treatment in classical state space models (use TBATS instead of ARIMA)
- here, we just choose the length of the "outer" period as state size:

&nbsp;

```{r, eval=FALSE, echo = TRUE}
lstm_units <- 24 * 7 # hourly * weekly
model %>% 
  layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features),
               return_sequences = TRUE) %>% 
  time_distributed(layer_dense(units = 1)) %>% 
  compile(
    loss = 'mean_squared_error',
    optimizer = 'adam'
)
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



168-step-ahead forecasts!
========================================================
class:smallcode

&nbsp;

And here are the forecasts, 168 steps ahead (displaying 3 only)
&nbsp;

```{r, results="hide"}
internet_train <- traffic_df$bits[1:800]
internet_test <- traffic_df$bits[801:nrow(traffic_df)]

model_exists <- TRUE

lstm_num_predictions <- 168
lstm_num_timesteps <- 168
batch_size <- 1
epochs <- 500
lstm_units <- 32
lstm_type <- "stateless"
data_type <- "data_scaled"
test_type <- "INTERNET"
model_type <- "model_lstm_time_distributed"

model_name <- "model_lstm_time_distributed_stateless_data_scaled_internet_timesteps_168"

train <- internet_train[!is.na(internet_train)]
test <- internet_test[!is.na(internet_test)]

num_train <- length(train)
num_test <- length(test)
num_all <- num_train + num_test

# normalize
minval <- min(train)
maxval <- max(train)

train <- normalize(train, minval, maxval)
test <- normalize(test, minval, maxval)

matrix_train <- build_matrix(train, lstm_num_timesteps + lstm_num_predictions) 
matrix_test <- build_matrix(test, lstm_num_timesteps + lstm_num_predictions) 

X_train <- matrix_train[ ,1:lstm_num_timesteps]
y_train <- matrix_train[ ,(lstm_num_timesteps + 1):(lstm_num_timesteps + lstm_num_predictions)]

X_test <- matrix_test[ ,1:lstm_num_timesteps]
y_test <- matrix_test[ ,(lstm_num_timesteps + 1):(lstm_num_timesteps + lstm_num_predictions)]

# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)
X_train <- reshape_X_3d(X_train)
X_test <- reshape_X_3d(X_test)

y_train <- reshape_X_3d(y_train)
y_test <- reshape_X_3d(y_test)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

# model
if (!model_exists) {
  set.seed(22222)
  model <- keras_model_sequential() 
  model %>% 
    layer_lstm(units = lstm_units, input_shape = c(num_steps, num_features),
               return_sequences = TRUE) %>% 
    time_distributed(layer_dense(units = 1)) %>% 
    compile(
      loss = 'mean_squared_error',
      optimizer = 'adam'
    )
  
  model %>% summary()
  
  model %>% fit( 
    X_train, y_train, batch_size = batch_size, epochs = epochs,
    validation_data = list(X_test, y_test), callbacks = callback_early_stopping(patience=2)
  )
  model %>% save_model_hdf5(filepath = paste0(model_name, ".h5"))
} else {
  model <- load_model_hdf5(filepath = paste0(model_name, ".h5"))
}

pred_train <- model %>% predict(X_train, batch_size = 1)
pred_test <- model %>% predict(X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train <- pred_train[ , , 1]
pred_test <- pred_test[ , , 1]

df <- data_frame(time_id = 1:length(test),
                 test = denormalize(test, minval, maxval))
for(i in seq_len(nrow(pred_test))) {
  varname <- paste0("pred_test", i)
  df <- mutate(df, !!varname := c(rep(NA, lstm_num_timesteps),
                                  rep(NA, i-1),
                                  pred_test[i, ],
                                  rep(NA, num_test - lstm_num_predictions - lstm_num_timesteps -i +1)))
}

ggplot(df, aes(x = time_id, y =test)) + geom_line()  +
 geom_line(aes(y = pred_test1), color = "cyan") + 
  geom_line(aes(y = pred_test48), color = "red") + 
  geom_line(aes(y = pred_test96), color = "green")  + 
  ggtitle("Internet traffic  in bits. 168-step-ahead forecasts for the test set.") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Takeaways
========================================================
class:smallcode

&nbsp;

- Time series forecasting is not just ARIMA, exponential smoothing & co.
- Especially for dynamically changing data, interesting alternatives are DLMs and Deep Learning
- Lots of opportunities for experimentation... let's find the unicorn :-)

&nbsp;

Have fun!





<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

