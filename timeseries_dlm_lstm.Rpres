<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
  hyphens: none;
}

.reveal h3 {
  font-size: 1.4em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.smallcode pre code {
  font-size: 0.9em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

.reveal .mediumtext {
  font-size: 0.85em;
}

</style>


Dynamic forecasts - with Bayesian linear models and neural networks
========================================================
author: Sigrid Keydana, Trivadis
date: 2017/14/11
autosize: true
incremental:false
width: 1600
height: 900

About me & my employer
========================================================
class:mediumtext


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Trivadis
- DACH-based IT consulting and service company, from traditional technologies to big data/machine learning/data science

My background
- from psychology/statistics via software development and database engineering to data science and ML/DL

My passion
- machine learning and deep learning
- data science and (Bayesian) statistics
- explanation/understanding over prediction accuracy

Where to find me
- blog: http://recurrentnull.wordpress.com
- twitter: @zkajdan


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Dynamic models for timeseries prediction, - why?
</h1>


Our task today: forecast men's 400m Olympic winning times
========================================================

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=16, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE,
                      cache = TRUE)

options(digits = 4, scipen = 4)
```

```{r}
library(ggplot2)
library(dplyr)
library(forecast)
library(tidyr)
library(keras)
library(R.matlab)
library(dlm)
library(quantmod)
library(gridExtra)

bold_text_20 <- element_text(face = "bold", size = 20)
bold_text_16 <- element_text(face = "bold", size = 16)
```


It's 2000, just before the Olympics. This is the data we have:

```{r}
data <- readMat("olympics.mat")
male400 <- data$male400[ ,1:2]
male400 <- as.data.frame(male400) %>% rename(year = V1, seconds = V2)
male400 <- male400 %>% bind_rows(
  c(year = 2012, seconds = 43.94), c(year = 2016, seconds = 43.03))

# data up till 1996
male400_1996 <- male400 %>% filter(year < 1997)

# data from 2000
male400_2000 <- male400 %>% filter(year > 1997)

ggplot(male400_1996, aes(x = year, y = seconds)) + geom_line() + ggtitle("Men's 400m Olympic winning times 1986-1996") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)


```




This looks like we might even try linear regression...
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

```{r, echo=TRUE}
lm(seconds ~ year, male400_1996) %>% coefficients()
```

```{r}
fit <- lm(seconds ~ year, male400_1996)
df <- male400_1996 %>% bind_rows(data.frame(year = seq(2000, 2016, by=4)))
preds <- fit %>% predict(newdata = df[25:29, ], interval = "prediction")
df <- df %>% bind_cols(data.frame(pred = c(rep(NA, 24), preds[ , 1]),
                                  lwr = c(rep(NA, 24), preds[ , 2]),
                                  upr = c(rep(NA, 24), preds[ , 3])))

ggplot(df, aes(x = year, y=seconds)) + geom_line() + 
  geom_point(aes(y = pred), color = "green", size = 3) +
  geom_ribbon(aes(x = year, ymin = lwr, ymax = upr), alpha = 0.1) +
  ggtitle("Men's 400m Olympic winning times 1986-1996 and predictions for 2000-2016") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")
```


Unfortunately...
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

```{r}
fit <- lm(seconds ~ year, male400_1996)
df <- male400 
preds <- fit %>% predict(newdata = df[25:29, ], interval = "prediction")
df <- df %>% bind_cols(data.frame(pred = c(rep(NA, 24), preds[ , 1]),
                                  lwr = c(rep(NA, 24), preds[ , 2]),
                                  upr = c(rep(NA, 24), preds[ , 3])))

ggplot(df, aes(x = year, y=seconds)) + geom_line() + 
  geom_point(aes(y = pred), color = "red", size = 3) +
  geom_ribbon(aes(x = year, ymin = lwr, ymax = upr, alpha = 0.1)) +
  ggtitle("Men's 400m Olympic winning times 1986-2016 and predictions for 2000-2016") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")
```

OK then... let's just use ARIMA
========================================================
class:smallcode

&nbsp;

(just take this cum grano salis as the series is not 100% regular)

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

```{r, echo=TRUE}
arima_fit <- auto.arima(male400_1996$seconds)
arima_fit$coef
```


```{r}
autoplot(forecast(arima_fit, h=5)) + 
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16, legend.position = "none")
```



What if... 
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

What if we didn't have to:

- use static coefficients?

or even:
- model linear relationships?


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



No static coefficients needed: Dynamic Linear Models (DLMs)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src='dlm_diag.png' width = 50%/>
</figure>


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Deep and non-linear: LSTM networks
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

pic lstm----

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Dynamic linear models
</h1>


State space models
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src="hmm.png" width='40%'/>
    <figcaption>Source: Wikipedia</figcaption>
</figure>

&nbsp;

&nbsp;

States form a Markov chain.

Observations are independent conditional on states.

Hidden states, observations, and time
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- Hidden states evolve in time (state equation):

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\theta_t = G_t\theta_{t-1} + w_t$

- Hidden states generate observables (observation equation):

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Y_t = F_t\theta_t + v_t$

- System noise is normally distributed with mean $0$ and variance $W_t$: 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_t \sim \mathcal{N}(0, W_t)$

- Observation noise is normally distributed with mean $0$ and variance $V_t$: 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $v_t \sim \mathcal{N}(0, V_t)$



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


How is this dynamic?
========================================================
class:smallcode

&nbsp;

In theory, any and all of 

- system noise
- observation noise
- state equation
- observation equation 

could change over time.


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;


Example: Where's the unicorn?
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- All we have is a sequence of (noisy) observations
- And a sequence of hypothesized true positions (states)

How can we optimise our assumptions about the true position?

<figure>
    <img src='unicorn.png' style="opacity: 0.1;" width="80%"/>
</figure>

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Bayesian updating
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- At any time, we have a current best estimate on where the unicorn is (given past observations)
- We compute the next true position (state) based on the _state equation_ (this is the _prior_)
- We predict the corresponding observation based on the _observation equation_ (this is the _likelihood_)
- As soon as the new observation arrives, we can correct our estimate and compute the _posterior_ on the state

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Show me the math: Kalman filter
========================================================
class:smalltext

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />

&nbsp;

Here's the model again

- state equation $\theta_t = G_t\theta_{t-1} + w_t$, where $w_t \sim \mathcal{N}(0, W_t)$
- observation equation $Y_t = F_t\theta_t + v_t$, where $v_t \sim \mathcal{N}(0, V_t)$
- prior $\theta_0 \sim \mathcal{N}_p(m_0, C_0)$

&nbsp;

Kalman steps (given: $\theta_{t-1}|y_{1:t-1} \sim \mathcal{N}(m_{t-1}, C_{t-1})$)

  1. One-step-ahead predictive distribution of $\theta_t$ given $y_{1:t-1}$:
  
  $E(\theta_t|y_{1:t-1}) = a_t = G_tm_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Var(\theta_t|y_{1:t-1}) = R_t = G_tC_{t-1}G'_t + W_t$

  2.  One-step-ahead predictive distribution of $Y_t$ given $y_{1:t-1}$:
  
  $E(Y_t|y_{1:t-1}) = f_t = F_t a_t$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Var(Y_t|y_{1:t-1}) = Q_t = F_tR_tF'_t + V_t$

  3. Filtering distribution of $\theta_t$ given $y_{1:t}$:
  
  $E(\theta_t|y_{1:t}) = m_t = a_t + R_tF'_tQ_t^{-1}e_t$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Var(\theta_t|y_{1:t}) = C_t = R_t - R_tF'_tQ_t^{-1}F_tR_t$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; where $e_t =  Y_t - f_t$ is the forecast error.



Kalman filter in a nutshell
========================================================
class:smallcode


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

> We correct our prediction by how much it differs from the incoming observation.

&nbsp;
&nbsp;

Posterior estimate &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $E(\theta_t|y_{1:t}) = m_t = a_t + R_tF'_tQ_t^{-1}e_t$


Kalman gain &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $R_tF'_tQ_t^{-1}$


Aside: Filtering vs. smoothing
========================================================
class:smallcode

&nbsp;

- In _filtering_, we base our estimates of the true states on all observations made _so far_.
- In _smoothing_, we form our opinion about the states "in hindsight", based on all past and (available) future states.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;



Local level model ("random walk plus noise")
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

State equation: $\mu_t = \mu_{t-1} + w_t, w_t \sim \mathcal{N}(0, W)$

Observation equation: $Y_t = \mu_t + v_t, v_t \sim \mathcal{N}(0, V)$

$G = F = 1$

&nbsp;

&nbsp;

W and V could be time-invariant or time-dependent.


Local level example
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;



```{r}
lakeSup <- ts(read.table("Datasets/lakeSuperior.dat", skip = 3,
                         colClasses = "numeric")[, 2], start = 1900)
```

```{r, echo=TRUE}
library(dlm)

build <- function(params) {
  dlmModPoly(order = 1, dV = exp(params[1]), dW = exp(params[2]))
}
initial_params <- rep(log(var(diff(lakeSup))), 2)
mle <- dlmMLE(lakeSup, initial_params, build)

model <- dlmModPoly(order = 1, dV = exp(mle$par[1]), dW = exp(mle$par[2]))
Map(function(x) round(x,2), unlist(model)) %>% unlist()
```


```{r, fig.width=10, fig.height=5}
autoplot(lakeSup) + ggtitle("Annual precipitation at Lake Superior, 1990-1986") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)

```


&nbsp;


Local level example: Filtering and smoothing
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;


```{r}
filt <- dlmFilter(lakeSup, model)
smooth <- dlmSmooth(lakeSup, model)
df <- data.frame(year = time(lakeSup),
                 actual = unclass(lakeSup),
                 filtered = dropFirst(filt$m),
                 smoothed = dropFirst(smooth$s))
df <- gather(df, key = type, value = value, -year)
ggplot(df, aes(x = year, y = value, color = type)) + geom_line() + 
  ggtitle("Annual precipitation at Lake Superior: filtering and smoothing estimates") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```


Two types of uncertainty
========================================================
class:smallcode

&nbsp;

How would the filtering and smoothing curves have looked had we specified equal degrees of system and observation noise?

&nbsp;

```{r, echo = TRUE}
model2 <- dlmModPoly(order = 1, dV = 10, dW = 10)
filt2 <- dlmFilter(lakeSup, model2)
smooth2 <- dlmSmooth(lakeSup, model2)
```

```{r}
df <- data.frame(year = time(lakeSup),
                 actual = unclass(lakeSup),
                 filtered = dropFirst(filt2$m),
                 smoothed = dropFirst(smooth2$s))
df <- gather(df, key = type, value = value, -year)
ggplot(df, aes(x = year, y = value, color = type)) + geom_line() + 
  ggtitle("Annual precipitation at Lake Superior: filtering and smoothing estimates") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;




Local level example: Forecasting 
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

```{r}
build <- function(params) {
  dlmModPoly(order = 1, dV = exp(params[1]), dW = exp(params[2]))
}
initial_params <- rep(log(var(diff(lakeSup))), 2)
mle <- dlmMLE(lakeSup, initial_params, build)

model <- dlmModPoly(order = 1, dV = exp(mle$par[1]), dW = exp(mle$par[2]))
filt <- dlmFilter(lakeSup, model)
fc <- dlmForecast(filt, nAhead = 10)
fc_sd <- sqrt(unlist(fc$Q))
lower <- fc$f - qnorm(0.025, lower = FALSE) * fc_sd
upper <- fc$f + qnorm(0.025, lower = FALSE) * fc_sd
df <- data.frame(year = c(time(lakeSup), 1987:1996),
                 value = c(unclass(lakeSup), rep(NA,10)),
                 forecast = c(rep(NA,87), fc$f),
                 lower = c(rep(NA,87), lower),
                 upper = c(rep(NA,87), upper))
ggplot(df, aes(x = year, y = value)) + geom_line() + 
  geom_line(aes(y = forecast), color = "red") + 
  geom_ribbon(aes(ymin = df$lower, ymax = df$upper), alpha = .3) +
  ggtitle("Annual precipitation at Lake Superior: forecasts with 95% prediction intervals") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```




Another local level example: Nvidia stock prices
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



```{r}
nvda <- getSymbols("NVDA", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$NVDA.Adjusted
index <- index(nvda)
nvda <- as.ts(nvda)
log_var_diff <- log(var(diff(nvda)))
```

```{r, echo=TRUE}
build <- function(params) {
  dlmModPoly(order = 1, dV = exp(params[1]), dW = exp(params[2]))
}
initial_params <- rep(log_var_diff, 2)
mle <- dlmMLE(nvda, initial_params, build)

model <- dlmModPoly(order = 1, dV = exp(mle$par[1]), dW = exp(mle$par[2]))
Map(function(x) round(x,2), unlist(model)) %>% unlist()
```


```{r, fig.width=10, fig.height=5}
autoplot(nvda) + ggtitle("Nvidia stock prices, Jan.-Sept. 2017") + theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
```


Nvidia: filtering and smoothing
========================================================
class:smallcode

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


```{r}
filt <- dlmFilter(nvda, model)
smooth <- dlmSmooth(nvda, model)
df <- data.frame(day = time(nvda),
                 actual = unclass(nvda),
                 filtered = dropFirst(filt$m),
                 smoothed = dropFirst(smooth$s))
df <- gather(df, key = type, value = value, -day)
ggplot(df, aes(x = day, y = value, color = type)) + geom_line() + 
  ggtitle("Nvidia stock prices: filtering and smoothing estimates") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```


(No, the lines did not turn invisible...)


Nvidia: Forecasting (or, let's beat the market)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

```{r}
build <- function(params) {
  dlmModPoly(order = 1, dV = exp(params[1]), dW = exp(params[2]))
}
initial_params <- rep(log_var_diff, 2)
mle <- dlmMLE(nvda, initial_params, build)

model <- dlmModPoly(order = 1, dV = exp(mle$par[1]), dW = exp(mle$par[2]))
filt <- dlmFilter(nvda, model)

fc <- dlmForecast(filt, nAhead = 5)
fc_sd <- sqrt(unlist(fc$Q))
lower <- fc$f - qnorm(0.025, lower = FALSE) * fc_sd
upper <- fc$f + qnorm(0.025, lower = FALSE) * fc_sd
df <- data.frame(day = c(time(nvda), 189:193),
                 actual = c(unclass(nvda), rep(NA,5)),
                 forecast = c(rep(NA,188), fc$f),
                 lower = c(rep(NA,188), lower),
                 upper = c(rep(NA,188), upper))
ggplot(df, aes(x = day, y = actual)) + geom_line() + 
  geom_line(aes(y = forecast), color = "red") + 
  geom_ribbon(aes(ymin = df$lower, ymax = df$upper), alpha = .3) +
  ggtitle("Nvidia: forecasts with 95% prediction intervals") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  
```


Not with this we won't...


DLMs - besides the local level model
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- linear trend model 
- seasonal models 
- regression models (with X as observation-generating matrix F and the coefficients as hidden state)
- multivariate models, exploiting underlying commonalities ("seemingly unrelated time series equations")
- multivariate regression models ("seemingly unrelated regression models")

Again, system and observation noise (as well as the transition equations) could be time-invariant or changing over time.


Nvidia revisited: Exploring relationships
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Comparing Nvidia to AMD, Mu, and IBM:

```{r}
amd <- getSymbols("AMD", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$AMD.Adjusted
nvda <- getSymbols("NVDA", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$NVDA.Adjusted
mu <- getSymbols("MU", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$MU.Adjusted
ibm <- getSymbols("IBM", from="2017-01-01", to="2017-10-01", auto.assign = FALSE)$IBM.Adjusted

df <- data.frame(index(amd), amd, mu, ibm, nvda)
tss <- read.zoo(df)
autoplot(tss) + facet_grid(Series ~ ., scales = "free_y") + ggtitle("")  +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
  

```


Does it look like Nvidia and IBM have a somewhat "complementary" relationship?


Nvidia and IBM: Linear regression
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;


```{r, echo=TRUE}
lm(nvda ~ ibm) %>% summary()
```


So there's a strong relationship...
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

... but is it really _constant over time_?

```{r, echo=TRUE}
lm(nvda ~ ibm) %>% residuals() %>% qqnorm()
```

&nbsp;





DLM regression to the rescue!
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

```{r, echo=TRUE}
build <- function(u) {
  dlmModReg(ibm, dV = exp(u[1]), dW = exp(u[2 : 3]))
}
mle <- dlmMLE(nvda, parm = rep(0, 3), build)
model <- build(mle$par)

model$FF
model$GG
model$W
model$V
```



Dynamic regression coefficients
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Final coefficients:

```{r}
filt <-dlmFilter(nvda, model)
filt$m[1 + length(nvda), ] # final coefficients
```

&nbsp;

Development of coefficients over time (except initial noise):

```{r}
intercept <- autoplot(as.ts(filt$m[ ,1])) + ggtitle("Intercept over time")  + ylab("intercept") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16) + coord_cartesian(ylim=c(90,200))
slope <-  autoplot(as.ts(filt$m[ ,2])) + ggtitle("Slope over time")  + ylab("slope") + coord_cartesian(ylim=c(-0.5, 0.5)) +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
grid.arrange(intercept, slope, ncol=2)
```


Back to the Olympics question...
========================================================

&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Let's model this as a dynamic regression problem!

```{r, echo=TRUE}
build <- function(u) {
  dlmModReg(male400_1996$year, dV = exp(u[1]), dW = exp(u[2 : 3]))
}
mle <- dlmMLE(male400_1996$seconds, parm = rep(0, 3), build)
model <- build(mle$par)
model$FF
model$GG
model$W
model$V

filt <-dlmFilter(male400_1996$seconds, model)
intercept <- autoplot(as.ts(filt$m[ ,1])) + ggtitle("Intercept over time")  + ylab("intercept") +
  theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16) #+ coord_cartesian(ylim=c(90,200))
slope <-  autoplot(as.ts(filt$m[ ,2])) + ggtitle("Slope over time")  + ylab("slope") #+ coord_cartesian(ylim=c(-0.5, 0.5)) +
  #theme(title = bold_text_16, axis.title = bold_text_16, axis.text = bold_text_16)
grid.arrange(intercept, slope, ncol=2)
```



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Deep Learning: Long Short Term Momory
</h1>



